{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "from KGAT import KGAT\n",
    "from utils.parser_kgat import *\n",
    "from utils.log_helper import *\n",
    "from utils.metrics import *\n",
    "from utils.model_helper import *\n",
    "from data_loader import DataLoaderKGAT\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import copy\n",
    "\n",
    "\n",
    "# Original code\n",
    "def evaluate(model, dataloader, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n",
    "    user_ids_batches = [torch.LongTensor(d) for d in user_ids_batches]\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "\n",
    "    cf_scores = []\n",
    "    metric_names = ['precision', 'recall', 'ndcg']\n",
    "    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n",
    "\n",
    "    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n",
    "        for batch_user_ids in user_ids_batches:\n",
    "            batch_user_ids = batch_user_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_scores = model(batch_user_ids, item_ids, mode='predict')       # (n_batch_users, n_items)\n",
    "\n",
    "            batch_scores = batch_scores.cpu()\n",
    "            batch_metrics = calc_metrics_at_k(batch_scores, train_user_dict, test_user_dict, batch_user_ids.cpu().numpy(), item_ids.cpu().numpy(), Ks)\n",
    "\n",
    "            cf_scores.append(batch_scores.numpy())\n",
    "            for k in Ks:\n",
    "                for m in metric_names:\n",
    "                    metrics_dict[k][m].append(batch_metrics[k][m])\n",
    "            pbar.update(1)\n",
    "\n",
    "    cf_scores = np.concatenate(cf_scores, axis=0)\n",
    "    for k in Ks:\n",
    "        for m in metric_names:\n",
    "            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n",
    "    return cf_scores, metrics_dict\n",
    "\n",
    "def predict(args):\n",
    "    # GPU / CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load data\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "\n",
    "    # load model\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations)\n",
    "    checkpoint = torch.load(args.pretrain_model_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # predict\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    cf_scores, metrics_dict = evaluate(model, data, Ks, device)\n",
    "    np.save(args.save_dir + 'cf_scores.npy', cf_scores)\n",
    "    print('CF Evaluation: Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kgat args (can adjust Ks and pretrain_model_path)\n",
    "class KGAT_args():\n",
    "    def __init__(self, \n",
    "                 seed=2019,\n",
    "                 data_name='amazon-book', \n",
    "                 data_dir='datasets/', \n",
    "                 use_pretrain=1, \n",
    "                 pretrain_embedding_dir='datasets/pretrain/', \n",
    "                 pretrain_model_path='trained_model/KGAT/model_epoch280.pth', \n",
    "                 cf_batch_size=1024, \n",
    "                 kg_batch_size=2048, \n",
    "                 test_batch_size=10000, \n",
    "                 embed_dim=64, \n",
    "                 relation_dim=64, \n",
    "                 laplacian_type='random-walk', \n",
    "                 aggregation_type='bi-interaction', \n",
    "                 conv_dim_list='[64, 32, 16]', \n",
    "                 mess_dropout='[0.1, 0.1, 0.1]', \n",
    "                 kg_l2loss_lambda=1e-5, \n",
    "                 cf_l2loss_lambda=1e-5, \n",
    "                 lr=0.0001, \n",
    "                 n_epoch=1000, \n",
    "                 stopping_steps=10, \n",
    "                 cf_print_every=1, \n",
    "                 kg_print_every=1, \n",
    "                 evaluate_every=20, \n",
    "                 Ks='[10]'):\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.data_name = data_name\n",
    "        self.data_dir = data_dir\n",
    "        self.use_pretrain = use_pretrain\n",
    "        self.pretrain_embedding_dir = pretrain_embedding_dir\n",
    "        self.pretrain_model_path = pretrain_model_path\n",
    "        self.cf_batch_size = cf_batch_size\n",
    "        self.kg_batch_size = kg_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.relation_dim = relation_dim\n",
    "        self.laplacian_type = laplacian_type\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.conv_dim_list = conv_dim_list\n",
    "        self.mess_dropout = mess_dropout\n",
    "        self.kg_l2loss_lambda = kg_l2loss_lambda\n",
    "        self.cf_l2loss_lambda = cf_l2loss_lambda\n",
    "        self.lr = lr\n",
    "        self.n_epoch = n_epoch\n",
    "        self.stopping_steps = stopping_steps\n",
    "        self.cf_print_every = cf_print_every\n",
    "        self.kg_print_every = kg_print_every\n",
    "        self.evaluate_every = evaluate_every\n",
    "        self.Ks = Ks\n",
    "        save_dir = 'trained_model/KGAT/{}/embed-dim{}_relation-dim{}_{}_{}_{}_lr{}_pretrain{}/'.format(\n",
    "        self.data_name, self.embed_dim, self.relation_dim, self.laplacian_type, self.aggregation_type,\n",
    "        '-'.join([str(i) for i in eval(self.conv_dim_list)]), self.lr, self.use_pretrain)\n",
    "        self.save_dir = save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = KGAT_args()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "data = DataLoaderKGAT(args, logging)\n",
    "\n",
    "# load model\n",
    "model = KGAT(args, data.n_users, data.n_entities, data.n_relations)\n",
    "checkpoint = torch.load(args.pretrain_model_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts\n",
    "print(data.n_items) # 0~24914\n",
    "print(data.n_entities) # 24915~113486\n",
    "print(data.n_users_entities) # 113487~184165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_path(start, end, target_r_1, target_r_2, adj_matrix):\n",
    "    adj_first = adj_matrix[0].tocsr()\n",
    "    adj_second = adj_matrix[target_r_1].tocsr()\n",
    "    #adj_last = adj_matrix[target_r_2].tocsr()\n",
    "    \n",
    "    if target_r_2 == 0:\n",
    "        reverse_adj_last = adj_matrix[1].tocsr()\n",
    "    elif target_r_2 <= 40:\n",
    "        reverse_adj_last = adj_matrix[target_r_2 + 39].tocsr()\n",
    "    else:\n",
    "        reverse_adj_last = adj_matrix[target_r_2 - 39].tocsr()\n",
    "    \n",
    "    # All items connect to start\n",
    "    first_step_candidate = adj_first.indices[adj_first.indptr[start]:adj_first.indptr[start+1]]\n",
    "    first_step_candidate = np.delete(first_step_candidate, np.where(first_step_candidate == end)[0])\n",
    "    \n",
    "    # All entity(uiei)/user(uiui) connect to end\n",
    "    last_step_candidate = reverse_adj_last.indices[reverse_adj_last.indptr[end]:reverse_adj_last.indptr[end+1]]\n",
    "    last_step_candidate = np.delete(last_step_candidate, np.where(last_step_candidate == start)[0])\n",
    "\n",
    "    output_paths = []\n",
    "    for h in first_step_candidate:\n",
    "        tails = adj_second.indices[adj_second.indptr[h]:adj_second.indptr[h+1]]\n",
    "        for t in tails:\n",
    "            if t in last_step_candidate:\n",
    "                output_paths.append([start, h, target_r_1, t, target_r_2, end])\n",
    "    \n",
    "    return output_paths\n",
    "\n",
    "def find_all_path_all_r(start, end, relations, adj_matrix):\n",
    "    all_paths = []\n",
    "    for r in relations:\n",
    "        if r == 1:\n",
    "            r_1 = 1\n",
    "            r_2 = 0\n",
    "        else:\n",
    "            r_1 = r\n",
    "            r_2 = r + 39\n",
    "        \n",
    "        #t1 = time.perf_counter()\n",
    "        paths = find_all_path(start, end, r_1, r_2, adj_matrix)\n",
    "        all_paths += paths\n",
    "        #t2 = time.perf_counter()\n",
    "        #print(\"t = \", t2 - t1)\n",
    "        paths = find_all_path(start, end, r_2, r_1, adj_matrix)\n",
    "        all_paths += paths\n",
    "        #t3 = time.perf_counter()\n",
    "        #print(\"t2 = \", t3 - t2)\n",
    "    return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the score of a single path \n",
    "# 'sum', 'mul', 'max'\n",
    "def path_score(path, A_in, mode='sum'):\n",
    "    assert len(path) == 6  # [U, I, r_1, E, r_2, I]\n",
    "\n",
    "    if mode == 'sum':\n",
    "        return A_in[path[0]][path[1]] + A_in[path[1]][path[3]] + A_in[path[3]][path[5]]\n",
    "    \n",
    "    elif mode == 'mul':\n",
    "        return A_in[path[0]][path[1]] * A_in[path[1]][path[3]] * A_in[path[3]][path[5]]\n",
    "    \n",
    "    elif mode == 'max':\n",
    "        return max(A_in[path[0]][path[1]], A_in[path[1]][path[3]], A_in[path[3]][path[5]])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('mode should be in [\"sum\", \"mul\", \"max\"].')\n",
    "\n",
    "# Rerank the paths\n",
    "def get_top_k_path(paths, A_in, mode='sum', k=5):\n",
    "    assert len(paths) > 0\n",
    "    \n",
    "    scores = [path_score(path, A_in, mode) for path in paths]\n",
    "    reranked_paths = [path for _, path in sorted(zip(scores,paths), reverse=True)]\n",
    "    sorted_scores = sorted(scores, reverse=True)\n",
    "\n",
    "    return reranked_paths[:k], sorted_scores[:k]\n",
    "\n",
    "# Single user prediction\n",
    "def predict_single(model, dataloader, user_id, relations, Ks, device):\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "\n",
    "    model.eval()\n",
    "    input_user_id = torch.LongTensor([user_id]).to(device)\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        matching_scores = model(input_user_id, item_ids, mode='predict')[0].cpu().numpy()       # (n_batch_users, n_items)\n",
    "\n",
    "    train_pos_item_list = train_user_dict[user_id]\n",
    "    matching_scores[train_pos_item_list] = -np.inf\n",
    "    top_k_items = np.argsort(-matching_scores)[:k_max]\n",
    "    top_k_all_att_scores = []\n",
    "    top_k_all_paths = []\n",
    "    top_k_paths = []\n",
    "    top_k_att_scores = []\n",
    "    \n",
    "    for item_id in top_k_items:\n",
    "        paths = find_all_path_all_r(user_id, item_id, relations, dataloader.adjacency_dict)\n",
    "        reranked_paths, scores = get_top_k_path(paths, model.A_in, mode='max', k=5)\n",
    "        top_k_all_att_scores.append(scores)\n",
    "        top_k_all_paths.append(reranked_paths)\n",
    "        top_k_att_scores.append(scores[0])\n",
    "        top_k_paths.append(reranked_paths[0])\n",
    "    \n",
    "    output = {}\n",
    "    for k in Ks:\n",
    "        reranked_items = [id for _, id in sorted(zip(top_k_att_scores[:k], top_k_items[:k]), reverse=True)]\n",
    "        explainations = [path for _, path in sorted(zip(top_k_att_scores[:k], top_k_paths[:k]), reverse=True)]\n",
    "        output[k] = {'item_ids': reranked_items, 'explainations': explainations}\n",
    "    \n",
    "    return output, top_k_all_att_scores, top_k_all_paths, top_k_items\n",
    "\n",
    "# Check original model predcition hit the GT in top-k (the result is in hits_10.txt, hits_20.txt)\n",
    "def predict_ori(model, dataloader, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "    \n",
    "    model.eval()\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n",
    "    user_ids_batches = [torch.LongTensor(d) for d in user_ids_batches]\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    hit_users = []\n",
    "    count = 113487\n",
    "    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n",
    "        for batch_user_ids in user_ids_batches:\n",
    "            batch_user_ids = batch_user_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                matching_scores = model(batch_user_ids, item_ids, mode='predict')       # (n_batch_users, n_items)\n",
    "\n",
    "            batch_user_ids = batch_user_ids.cpu().numpy()\n",
    "            test_pos_item_binary = np.zeros([len(batch_user_ids), len(item_ids.cpu().numpy())], dtype=np.float32)\n",
    "            for idx, u in enumerate(batch_user_ids):\n",
    "                train_pos_item_list = train_user_dict[u]\n",
    "                test_pos_item_list = test_user_dict[u]\n",
    "                matching_scores[idx][train_pos_item_list] = -np.inf\n",
    "                test_pos_item_binary[idx][test_pos_item_list] = 1\n",
    "\n",
    "            try:\n",
    "                _, rank_indices = torch.sort(matching_scores.cuda(), descending=True)    # try to speed up the sorting process\n",
    "            except:\n",
    "                _, rank_indices = torch.sort(matching_scores, descending=True)\n",
    "\n",
    "            rank_indices = rank_indices.cpu()\n",
    "            binary_hit = []\n",
    "            for i in range(len(batch_user_ids)):\n",
    "                binary_hit.append(test_pos_item_binary[i][rank_indices[i]])\n",
    "            binary_hit = np.array(binary_hit, dtype=np.float32)[:, :k_max]\n",
    "            hit_user = np.where(np.sum(binary_hit, axis=1) > 0)[0]\n",
    "            hit_users += list(hit_user + count)\n",
    "            count += test_batch_size\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return hit_users\n",
    "\n",
    "# Get results of all users (explainable version)\n",
    "def explainable_evaluate(model, dataloader, relations, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "\n",
    "    metric_names = ['precision', 'recall', 'ndcg']\n",
    "    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n",
    "\n",
    "    with tqdm(total=len(user_ids), desc='Evaluating Iteration') as pbar:\n",
    "        for user_id in user_ids:\n",
    "\n",
    "            output, _, _ = predict_single(model, dataloader, user_id, relations, Ks, device)\n",
    "            batch_metrics = exp_calc_metrics_at_k(output[max(Ks)]['item_ids'], train_user_dict, test_user_dict, [user_id], item_ids.cpu().numpy(), Ks)\n",
    "\n",
    "            for k in Ks:\n",
    "                for m in metric_names:\n",
    "                    metrics_dict[k][m].append(batch_metrics[k][m])\n",
    "            pbar.update(1)\n",
    "\n",
    "    for k in Ks:\n",
    "        for m in metric_names:\n",
    "            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n",
    "    return metrics_dict\n",
    "    \n",
    "def explainable_recommend(model, data, relations, Ks):\n",
    "    # predict\n",
    "    Ks = eval(Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    metrics_dict = explainable_evaluate(model, data, relations, Ks, device)\n",
    "    print('CF Evaluation: Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Iteration: 100%|██████████| 8/8 [02:38<00:00, 19.79s/it]\n"
     ]
    }
   ],
   "source": [
    "#delete [2, 3, 6, 11, 21, 22, 32, 35, 37] -> [4, 5, 8, 13, 23, 24, 34, 37, 39] ([43, 44, 47, 52, 62, 63, 73, 76, 78])\n",
    "all_relations = range(2, 41)\n",
    "delete_relations = [4, 5, 8, 13, 23, 24, 34, 37, 39]\n",
    "target_relations = list(set(all_relations) - set(delete_relations))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "output, all_paths, all_scores, ori_top_k = predict_single(model, data, 113511, target_relations, [20], device)\n",
    "#hits = predict_ori(model, data, [10], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[805, 4693, 324, 1473, 548, 271, 2249, 291, 537, 2841, 642, 2142, 4016, 1078, 6897, 5401, 2040, 534, 6891, 4025]\n",
      "[6897, 6891, 5401, 4693, 4025, 4016, 2841, 2249, 2142, 2040, 1473, 1078, 805, 642, 548, 537, 534, 324, 291, 271]\n",
      "[  805 15038]\n"
     ]
    }
   ],
   "source": [
    "print(list(ori_top_k))\n",
    "print(output[20]['item_ids'])\n",
    "print(data.test_user_dict[113511])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
