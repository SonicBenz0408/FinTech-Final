{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "from KGAT import KGAT\n",
    "from utils.parser_kgat import *\n",
    "from utils.log_helper import *\n",
    "from utils.metrics import *\n",
    "from utils.model_helper import *\n",
    "from data_loader import DataLoaderKGAT\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import copy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n",
    "    user_ids_batches = [torch.LongTensor(d) for d in user_ids_batches]\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "\n",
    "    cf_scores = []\n",
    "    metric_names = ['precision', 'recall', 'ndcg']\n",
    "    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n",
    "\n",
    "    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n",
    "        for batch_user_ids in user_ids_batches:\n",
    "            batch_user_ids = batch_user_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_scores = model(batch_user_ids, item_ids, mode='predict')       # (n_batch_users, n_items)\n",
    "\n",
    "            batch_scores = batch_scores.cpu()\n",
    "            batch_metrics = calc_metrics_at_k(batch_scores, train_user_dict, test_user_dict, batch_user_ids.cpu().numpy(), item_ids.cpu().numpy(), Ks)\n",
    "\n",
    "            cf_scores.append(batch_scores.numpy())\n",
    "            for k in Ks:\n",
    "                for m in metric_names:\n",
    "                    metrics_dict[k][m].append(batch_metrics[k][m])\n",
    "            pbar.update(1)\n",
    "\n",
    "    cf_scores = np.concatenate(cf_scores, axis=0)\n",
    "    for k in Ks:\n",
    "        for m in metric_names:\n",
    "            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n",
    "    return cf_scores, metrics_dict\n",
    "\n",
    "def predict(args):\n",
    "    # GPU / CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load data\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "\n",
    "    # load model\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations)\n",
    "    load_model(model, args.pretrain_model_path)\n",
    "    model.to(device)\n",
    "\n",
    "    # predict\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    cf_scores, metrics_dict = evaluate(model, data, Ks, device)\n",
    "    np.save(args.save_dir + 'cf_scores.npy', cf_scores)\n",
    "    print('CF Evaluation: Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGAT_args():\n",
    "    def __init__(self, \n",
    "                 seed=2019,\n",
    "                 data_name='amazon-book', \n",
    "                 data_dir='datasets/', \n",
    "                 use_pretrain=1, \n",
    "                 pretrain_embedding_dir='datasets/pretrain/', \n",
    "                 pretrain_model_path='trained_model/KGAT/model_epoch280.pth', \n",
    "                 cf_batch_size=1024, \n",
    "                 kg_batch_size=2048, \n",
    "                 test_batch_size=10000, \n",
    "                 embed_dim=64, \n",
    "                 relation_dim=64, \n",
    "                 laplacian_type='random-walk', \n",
    "                 aggregation_type='bi-interaction', \n",
    "                 conv_dim_list='[64, 32, 16]', \n",
    "                 mess_dropout='[0.1, 0.1, 0.1]', \n",
    "                 kg_l2loss_lambda=1e-5, \n",
    "                 cf_l2loss_lambda=1e-5, \n",
    "                 lr=0.0001, \n",
    "                 n_epoch=1000, \n",
    "                 stopping_steps=10, \n",
    "                 cf_print_every=1, \n",
    "                 kg_print_every=1, \n",
    "                 evaluate_every=20, \n",
    "                 Ks='[20, 40, 60, 80, 100]'):\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.data_name = data_name\n",
    "        self.data_dir = data_dir\n",
    "        self.use_pretrain = use_pretrain\n",
    "        self.pretrain_embedding_dir = pretrain_embedding_dir\n",
    "        self.pretrain_model_path = pretrain_model_path\n",
    "        self.cf_batch_size = cf_batch_size\n",
    "        self.kg_batch_size = kg_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.relation_dim = relation_dim\n",
    "        self.laplacian_type = laplacian_type\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.conv_dim_list = conv_dim_list\n",
    "        self.mess_dropout = mess_dropout\n",
    "        self.kg_l2loss_lambda = kg_l2loss_lambda\n",
    "        self.cf_l2loss_lambda = cf_l2loss_lambda\n",
    "        self.lr = lr\n",
    "        self.n_epoch = n_epoch\n",
    "        self.stopping_steps = stopping_steps\n",
    "        self.cf_print_every = cf_print_every\n",
    "        self.kg_print_every = kg_print_every\n",
    "        self.evaluate_every = evaluate_every\n",
    "        self.Ks = Ks\n",
    "        save_dir = 'trained_model/KGAT/{}/embed-dim{}_relation-dim{}_{}_{}_{}_lr{}_pretrain{}/'.format(\n",
    "        self.data_name, self.embed_dim, self.relation_dim, self.laplacian_type, self.aggregation_type,\n",
    "        '-'.join([str(i) for i in eval(self.conv_dim_list)]), self.lr, self.use_pretrain)\n",
    "        self.save_dir = save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = KGAT_args()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "data = DataLoaderKGAT(args, logging)\n",
    "\n",
    "# load model\n",
    "model = KGAT(args, data.n_users, data.n_entities, data.n_relations)\n",
    "load_model(model, args.pretrain_model_path)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24915\n",
      "113487\n",
      "184166\n"
     ]
    }
   ],
   "source": [
    "print(data.n_items) # 0~24914\n",
    "print(data.n_entities) # 24915~113486\n",
    "print(data.n_users_entities) # 113487~184165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_path(start, end, target_r_1, target_r_2, adj_matrix):\n",
    "    adj_first = adj_matrix[0].tocsr()\n",
    "    adj_second = adj_matrix[target_r_1].tocsr()\n",
    "    adj_last = adj_matrix[target_r_2].tocsr()\n",
    "    \n",
    "    if target_r_2 == 0:\n",
    "        reverse_adj_last = adj_matrix[1].tocsr()\n",
    "    elif target_r_2 <= 40:\n",
    "        reverse_adj_last = adj_matrix[target_r_2 + 39].tocsr()\n",
    "    else:\n",
    "        reverse_adj_last = adj_matrix[target_r_2 - 39].tocsr()\n",
    "    \n",
    "    # All items connect to start\n",
    "    first_step_candidate = adj_first.indices[adj_first.indptr[start]:adj_first.indptr[start+1]]\n",
    "    first_step_candidate = np.delete(first_step_candidate, np.where(first_step_candidate == end)[0])\n",
    "    \n",
    "    # All entity(uiei)/user(uiui) connect to end\n",
    "    last_step_candidate = reverse_adj_last.indices[reverse_adj_last.indptr[end]:reverse_adj_last.indptr[end+1]]\n",
    "    last_step_candidate = np.delete(last_step_candidate, np.where(last_step_candidate == start)[0])\n",
    "\n",
    "    output_paths = []\n",
    "    for h in first_step_candidate:\n",
    "        tails = adj_second.indices[adj_second.indptr[h]:adj_second.indptr[h+1]]\n",
    "        for t in tails:\n",
    "            if t in last_step_candidate:\n",
    "                output_paths.append([start, h, target_r_1, t, target_r_2, end])\n",
    "    \n",
    "    if len(output_paths) > 0:\n",
    "        return output_paths\n",
    "    else:\n",
    "        #print(\"there is no path between them\")\n",
    "        return 0\n",
    "\n",
    "def find_all_path_all_r(start, end, relations, adj_matrix):\n",
    "    all_paths = []\n",
    "    for r in relations:\n",
    "        if r == 0:\n",
    "            r_1 = 0\n",
    "            r_2 = 1\n",
    "        elif r == 1:\n",
    "            r_1 = 1\n",
    "            r_2 = 0\n",
    "        elif r <= 40:\n",
    "            r_1 = r\n",
    "            r_2 = r + 39\n",
    "        else:\n",
    "            r_1 = r\n",
    "            r_2 = r - 39\n",
    "\n",
    "        paths = find_all_path(start, end, r_1, r_2, adj_matrix)\n",
    "        if paths == 0:\n",
    "            continue\n",
    "        if len(all_paths) == 0:\n",
    "            all_paths = paths\n",
    "        else:\n",
    "            all_paths += paths\n",
    "    return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24925], dtype=int32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete [2, 3, 6, 11, 21, 22, 32, 35, 37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[113490, 52, 24925, 54], [113490, 53, 24925, 54], [113490, 55, 24925, 54], [113490, 56, 24925, 54], [113490, 57, 24925, 54], [113490, 58, 24925, 54], [113490, 59, 24925, 54], [113490, 60, 24925, 54], [113490, 62, 24925, 54], [113490, 64, 24925, 54], [113490, 65, 24925, 54]]\n"
     ]
    }
   ],
   "source": [
    "user = 113490\n",
    "item = 54\n",
    "target_r_1 = 6\n",
    "target_r_2 = 45\n",
    "r_all_path = find_all_path(113490, 54, target_r_1, target_r_2, data.adjacency_dict)\n",
    "print(r_all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'sum', 'mul', 'max'\n",
    "def path_score(path, A_in, mode='sum'):\n",
    "    assert len(path) == 6  # [U, I, r_1, E, r_2, I]\n",
    "\n",
    "    if mode == 'sum':\n",
    "        return A_in[path[0]][path[1]] + A_in[path[1]][path[2]] + A_in[path[2]][path[3]]\n",
    "    \n",
    "    elif mode == 'mul':\n",
    "        return A_in[path[0]][path[1]] * A_in[path[1]][path[2]] * A_in[path[2]][path[3]]\n",
    "    \n",
    "    elif mode == 'max':\n",
    "        return max(A_in[path[0]][path[1]], A_in[path[1]][path[2]], A_in[path[2]][path[3]])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('mode should be in [\"sum\", \"mul\", \"max\"].')\n",
    "\n",
    "def get_top_k_path(paths, A_in, mode='sum', k=5):\n",
    "    assert len(paths) > 0\n",
    "    \n",
    "    scores = [path_score(path, A_in, mode) for path in paths]\n",
    "    reranked_paths = [path for _, path in sorted(zip(scores,paths), reverse=True)]\n",
    "    sorted_scores = sorted(scores, reverse=True)\n",
    "\n",
    "    return reranked_paths[:k], sorted_scores[:k]\n",
    "\n",
    "def predict_single(model, dataloader, user_id, relations, Ks, device):\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "\n",
    "    model.eval()\n",
    "    input_user_id = torch.LongTensor([user_id]).to(device)\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    print(f'Get matching score of user {user_id}......')\n",
    "    with torch.no_grad():\n",
    "        matching_scores = model(input_user_id, item_ids, mode='predict')[0].cpu().numpy()       # (n_batch_users, n_items)\n",
    "    \n",
    "    top_k_items = np.argsort(-matching_scores)[:k_max]\n",
    "    top_k_all_att_scores = []\n",
    "    top_k_all_paths = []\n",
    "    top_k_paths = []\n",
    "    top_k_att_scores = []\n",
    "    \n",
    "    for item_id in tqdm(top_k_items):\n",
    "        paths = find_all_path_all_r(user_id, item_id, relations, dataloader.adjacency_dict)\n",
    "        reranked_paths, scores = get_top_k_path(paths, model.A_in, mode='sum', k=5)\n",
    "        top_k_all_att_scores.append(scores)\n",
    "        top_k_all_paths.append(reranked_paths)\n",
    "        top_k_att_scores.append(scores[0])\n",
    "        top_k_paths.append(reranked_paths[0])\n",
    "    \n",
    "    output = {}\n",
    "    for k in Ks:\n",
    "        reranked_items = [id for _, id in sorted(zip(top_k_att_scores[:k], top_k_items[:k]), reverse=True)]\n",
    "        explainations = [path for _, path in sorted(zip(top_k_att_scores[:k], top_k_paths[:k]), reverse=True)]\n",
    "        output[k] = {'item_ids': reranked_items, 'explainations': explainations}\n",
    "    \n",
    "    return output, top_k_all_att_scores, top_k_all_paths\n",
    "\n",
    "def explainable_evaluate(model, dataloader, relations, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "\n",
    "    cf_scores = []\n",
    "    metric_names = ['precision', 'recall', 'ndcg']\n",
    "    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n",
    "\n",
    "    with tqdm(total=len(user_ids), desc='Evaluating Iteration') as pbar:\n",
    "        for user_id in user_ids:\n",
    "\n",
    "            rerank_indices, _, _ = predict_single(model, dataloader, user_id, relations, Ks, device)\n",
    "            batch_metrics = calc_metrics_at_k_exp(rerank_indices, train_user_dict, test_user_dict, [user_id], item_ids.cpu().numpy(), Ks)\n",
    "\n",
    "            for k in Ks:\n",
    "                for m in metric_names:\n",
    "                    metrics_dict[k][m].append(batch_metrics[k][m])\n",
    "            pbar.update(1)\n",
    "\n",
    "    for k in Ks:\n",
    "        for m in metric_names:\n",
    "            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n",
    "    return metrics_dict\n",
    "    \n",
    "def explainable_recommend(model, data, relations, Ks):\n",
    "    # predict\n",
    "    Ks = eval(Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    metrics_dict = explainable_evaluate(model, data, relations, Ks, device)\n",
    "    print('CF Evaluation: Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum ([[113490, 57, 24925, 54], [113490, 62, 24925, 54], [113490, 60, 24925, 54], [113490, 65, 24925, 54], [113490, 64, 24925, 54]], [tensor(1.1899, device='cuda:0'), tensor(1.0503, device='cuda:0'), tensor(1.0312, device='cuda:0'), tensor(1.0153, device='cuda:0'), tensor(1.0007, device='cuda:0')])\n",
      "mul ([[113490, 57, 24925, 54], [113490, 62, 24925, 54], [113490, 55, 24925, 54], [113490, 53, 24925, 54], [113490, 60, 24925, 54]], [tensor(3.5424e-07, device='cuda:0'), tensor(1.2501e-07, device='cuda:0'), tensor(9.5676e-08, device='cuda:0'), tensor(9.4368e-08, device='cuda:0'), tensor(7.1571e-08, device='cuda:0')])\n",
      "max ([[113490, 65, 24925, 54], [113490, 58, 24925, 54], [113490, 60, 24925, 54], [113490, 64, 24925, 54], [113490, 52, 24925, 54]], [tensor(0.9875, device='cuda:0'), tensor(0.9854, device='cuda:0'), tensor(0.9824, device='cuda:0'), tensor(0.9767, device='cuda:0'), tensor(0.9644, device='cuda:0')])\n"
     ]
    }
   ],
   "source": [
    "print('sum', get_top_k_path(r_all_path, model.A_in, mode='sum'))\n",
    "print('mul', get_top_k_path(r_all_path, model.A_in, mode='mul'))\n",
    "print('max', get_top_k_path(r_all_path, model.A_in, mode='max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get matching score of user 113490......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:46<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "#delete [2, 3, 6, 11, 21, 22, 32, 35, 37] -> [4, 5, 8, 13, 23, 24, 34, 37, 39] [43, 44, 47, 52, 62, 63, 73, 76, 78]\n",
    "all_relations = range(80)\n",
    "delete_relations = [4, 5, 8, 13, 23, 24, 34, 37, 39, 43, 44, 47, 52, 62, 63, 73, 76, 78]\n",
    "target_relations = list(set(all_relations) - set(delete_relations))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "output, all_paths, all_scores = predict_single(model, data, 113490, target_relations, [20], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(\"['20']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Iteration:   0%|          | 0/70591 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get matching score of user 113487......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:57<00:00,  2.89s/it]\n",
      "Evaluating Iteration:   0%|          | 0/70591 [00:58<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calc_metrics_at_k_exp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5536/802476662.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexplainable_recommend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_relations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[20]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5536/2790380136.py\u001b[0m in \u001b[0;36mexplainable_recommend\u001b[1;34m(model, data, relations, Ks)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mk_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mmetrics_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainable_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     print('CF Evaluation: Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n\u001b[0;32m    102\u001b[0m         metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5536/2790380136.py\u001b[0m in \u001b[0;36mexplainable_evaluate\u001b[1;34m(model, dataloader, relations, Ks, device)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mrerank_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mbatch_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_metrics_at_k_exp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrerank_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_user_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_user_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mKs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calc_metrics_at_k_exp' is not defined"
     ]
    }
   ],
   "source": [
    "explainable_recommend(model, data, target_relations, \"[20]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Iteration: 100%|██████████| 8/8 [03:49<00:00, 28.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF Evaluation: Precision [0.0150, 0.0071], Recall [0.1441, 0.3102], NDCG [0.0765, 0.1143]\n"
     ]
    }
   ],
   "source": [
    "predict(args)sum ([[113490, 57, 24925, 54], [113490, 62, 24925, 54], [113490, 60, 24925, 54], [113490, 65, 24925, 54], [113490, 64, 24925, 54]], [tensor(1.1899, device='cuda:0'), tensor(1.0503, device='cuda:0'), tensor(1.0312, device='cuda:0'), tensor(1.0153, device='cuda:0'), tensor(1.0007, device='cuda:0')])\n",
    "mul ([[113490, 57, 24925, 54], [113490, 62, 24925, 54], [113490, 55, 24925, 54], [113490, 53, 24925, 54], [113490, 60, 24925, 54]], [tensor(3.5424e-07, device='cuda:0'), tensor(1.2501e-07, device='cuda:0'), tensor(9.5676e-08, device='cuda:0'), tensor(9.4368e-08, device='cuda:0'), tensor(7.1571e-08, device='cuda:0')])\n",
    "max ([[113490, 65, 24925, 54], [113490, 58, 24925, 54], [113490, 60, 24925, 54], [113490, 64, 24925, 54], [113490, 52, 24925, 54]], [tensor(0.9875, device='cuda:0'), tensor(0.9854, device='cuda:0'), tensor(0.9824, device='cuda:0'), tensor(0.9767, device='cuda:0'), tensor(0.9644, device='cuda:0')])\n",
    "array([24925], dtype=int32)\n",
    "[[113490, 52, 24925, 54], [113490, 53, 24925, 54], [113490, 55, 24925, 54], [113490, 56, 24925, 54], [113490, 57, 24925, 54], [113490, 58, 24925, 54], [113490, 59, 24925, 54], [113490, 60, 24925, 54], [113490, 62, 24925, 54], [113490, 64, 24925, 54], [113490, 65, 24925, 54]]\n",
    "Evaluating Iteration:   0%|          | 0/70591 [00:00<?, ?it/s]Get matching score of user 113487......\n",
    "100%|██████████| 20/20 [00:57<00:00,  2.89s/it]\n",
    "Evaluating Iteration:   0%|          | 0/70591 [00:58<?, ?it/s]\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "~\\AppData\\Local\\Temp/ipykernel_5536/802476662.py in <module>\n",
    "----> 1 explainable_recommend(model, data, target_relations, \"[20]\")\n",
    "\n",
    "~\\AppData\\Local\\Temp/ipykernel_5536/2790380136.py in explainable_recommend(model, data, relations, Ks)\n",
    "     98     k_max = max(Ks)\n",
    "     99 \n",
    "--> 100     metrics_dict = explainable_evaluate(model, data, relations, Ks, device)\n",
    "    101     print('CF Evaluation: Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "    102         metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n",
    "\n",
    "~\\AppData\\Local\\Temp/ipykernel_5536/2790380136.py in explainable_evaluate(model, dataloader, relations, Ks, device)\n",
    "     80 \n",
    "     81             rerank_indices, _, _ = predict_single(model, dataloader, user_id, relations, Ks, device)\n",
    "---> 82             batch_metrics = calc_metrics_at_k_exp(rerank_indices, train_user_dict, test_user_dict, [user_id], item_ids.cpu().numpy(), Ks)\n",
    "     83 \n",
    "     84             for k in Ks:\n",
    "\n",
    "NameError: name 'calc_metrics_at_k_exp' is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6187b84b243d8ea189c25096ee879342dcb761226c806e7300ffe35952565784"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
